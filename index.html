<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Gang Zhou</title>
  <meta name="author" content="Yuheng Ji">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <style>
    .projects-box {
      height: 160px;
      position: relative;
      overflow: hidden;
      transition: all ease-in .1s;
    }
    .projects-show {
      padding: 0;
      margin: 0;
      position: absolute;
      bottom: 0;
      left: 0;
      text-align: center;
      height: 30px;
      line-height: 20px;
      width: 100%;
      font-size: 12px;
      color: #0067c8;
      cursor: pointer;
      background: linear-gradient(to bottom, transparent, #fff, #fff);
    }
    .projects-show-text {
      position: relative;
      top: 10px;
    }
    /* Custom styles for the layout */
    .profile-container {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin: 0 auto;
      max-width: 800px;
    }
    .profile-info {
      width: 60%;
      padding: 20px;
    }
    .profile-photo {
      width: 35%;
      text-align: center;
    }
    .profile-photo img {
      width: 100%;
      border-radius: 50%;
    }
    .links {
      text-align: center;
      margin-top: 10px;
    }
    .links a {
      margin: 0 10px;
      text-decoration: none;
      color: #0067c8;
    }
  </style>
</head>

<body>

	 <table width="100%" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="160" valign="top" align="center">
        <div class="one">
          <img src="your-photo.jpg" width="160" height="160" alt="Photo">
        </div>
        <div style="margin-top: 10px;">
          <div style="font-size: 18px; font-weight: bold;">Gang Zhou</div>
          <div style="font-size: 14px; color: #666;">School of Artificial Intelligence, Beijing University of Posts and Telecommunications</div>
        </div>
      </td>
      <td valign="top">
        <p>Welcome to my personal homepage! I am a Ph.D. student at the School of Artificial Intelligence, Beijing University of Posts and Telecommunications. I expect to graduate in 2027.</p>

        <!-- You can add more sections here, such as research interests, publications, CV, etc. -->
        
      </td>
    </tr>
  </table>

  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <h2>Research</h2>
          <p>My research focuses on trustworthy multimodal data retrieval and vision-language models.<br> * denotes equal contributions.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <!-- Paper List -->
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/reason_rft.png' width="180">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://tanhuajie.github.io/ReasonRFT/">
            <h3>Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning</h3>
          </a>
          <p>Huajie Tan*, <strong>Yuheng Ji*</strong>, Xiaoshuai Hao*, Minglan Lin, Pengwei Wang, Shanghang Zhang<br><em>arXiv</em>, 2025</p>
          <a href="https://tanhuajie.github.io/ReasonRFT/">Project</a> / <a href="https://tanhuajie.github.io/ReasonRFT/">Paper</a>
          <p>We developed <strong>Reason-RFT</strong>, a novel reinforcement fine-tuning framework that enhances visual reasoning capabilities in Vision-Language Models (VLMs). Reason-RFT employs a two-phase training strategy: (1) SFT with curated CoT data to activate reasoning potential, followed by (2) Group Relative Policy Optimization (GRPO)-based reinforcement learning to generate diverse reasoning-response pairs.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/robobrain.jpg' width="180" height="110">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://superrobobrain.github.io/">
            <h3>RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete</h3>
          </a>
          <p><strong>Yuheng Ji</strong>*, Huajie Tan*, Jiayu Shi*, Xiaoshuai Hao*, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang<br><em>CVPR</em>, 2025</p>
          <a href="https://superrobobrain.github.io/">Project</a> / <a href="https://superrobobrain.github.io/">Paper</a>
          <p>We developed <strong>RoboBrain</strong>, an MLLM-based model that combines robotic and general multi-modal data, utilizes a multi-stage training strategy, and incorporates long videos and high-resolution images to improve its robotic manipulation capabilities. Extensive experiments demonstrate that RoboBrain achieves state-of-the-art performance across various robotic tasks, highlighting its potential to advance robotic brain capabilities.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/msc-bench.png' width="180" height="100">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://msc-bench.github.io/">
            <h3>MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception</h3>
          </a>
          <p>Xiaoshuai Hao, Guanqun Liu, Yuting Zhao, <strong>Yuheng Ji</strong>, Mengchuan Wei, Haimei Zhao, Lingdong Kong, Rong Yin, Yu Liu<br><em>ICME</em>, 2025</p>
          <a href="https://msc-bench.github.io/">Project</a> / <a href="https://arxiv.org/abs/2501.01037">Paper</a>
          <p>This work introduces Multi-Sensor Corruption Benchmark (MSC-Bench), the first comprehensive benchmark aimed at evaluating the robustness of multi-sensor autonomous driving perception models against various sensor corruptions.</p>
        </td>
      </tr>
      <!-- Add more papers as needed -->
    </tbody>
  </table>

  <!-- Experience, Service, Award sections follow the same structure, no change needed here. -->

  <script>
    let show = false;
    document.querySelector('#projects-show').onclick = function() {
      if (!show) {
        document.querySelector('#projects-box').style.height = 'auto';
        document.querySelector('#projects-box').style.paddingBottom = '20px';
        document.querySelector('#projects-show-text').innerHTML = 'Less';
        show = true;
      } else {
        show = false;
        document.querySelector('#projects-box').style.height = '160px';
        document.querySelector('#projects-box').style.paddingBottom = '0px';
        document.querySelector('#projects-show-text').innerHTML = 'More';
      }
    }
  </script>
</body>
</html>
