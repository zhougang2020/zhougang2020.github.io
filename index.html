<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Gang Zhou</title>

  <meta name="author" content="Yuheng Ji">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
      <style>
    .projects-box {
      /* background-color: green; */
      height: 160px;
      position: relative;
      overflow: hidden;
      transition: all ease-in .1s;

    }
    .projects-show {
      padding: 0;
      margin: 0;
      position: absolute;
      bottom: 0;
      left: 0;
      text-align: center;
      height: 30px;
      line-height: 20px;
      width: 100%;
      font-size: 12px;
      color: #0067c8;
      cursor: pointer;
      background: linear-gradient(to bottom, transparent, #fff, #fff);
    }
    .projects-show-text {
      position: relative;
      top: 10px;
    }
  </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Gang Zhou (Âë®Âàö)</name>
              </p>

              <p style="text-align:justify">My name is Zhou Gang, and I am currently a doctoral student at the School of Artificial Intelligence, Beijing University of Posts and Telecommunications, expected to graduate in 2027. My research focuses on trustworthy multimodal data retrieval and vision-language models.
              </p>

              <p style="text-align:center">
                <a href="jiyuheng2023@ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=X4ILYUQAAAAJ&hl=en/">Google Scholar</a> &nbsp/&nbsp
		<a href="data/Â∞èÂ≤õÈõÜ.pdf">Poetry Anthology</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jiyuheng2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jiyuheng2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests primarily lie in embodied AI and computer vision.
                <br> * denotes equal contributions.
              </p>
            </td>
          </tr>
        </tbody></table>


<!--        paper_list-->
<!--------------------------------------------------------------------------------------------------------------------------->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
	<!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/reason_rft.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://tanhuajie.github.io/ReasonRFT/">
                  <papertitle>Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning</papertitle>
                </a>
                <br>
		Huajie Tan*,
                <strong>Yuheng Ji*</strong>,
		Xiaoshuai Hao*,
		Minglan Lin,
		Pengwei Wang,
		Shanghang Zhang
                <br>
                <em>arXiv</em>, 2025
                <br>
		<a href="https://tanhuajie.github.io/ReasonRFT/">Project</a>
		/
		<a href="https://tanhuajie.github.io/ReasonRFT/">Paper</a>
                <p></p>   
                <p style="text-align:justify">
                We developed <strong>Reason-RFT</strong>, a novel reinforcement fine-tuning framework that enhances visual reasoning capabilities in Vision-Language Models (VLMs). 
			Reason-RFT employs a two-phase training strategy: (1) SFT with curated CoT data to activate reasoning potential, followed by 
			(2) Group Relative Policy Optimization (GRPO)-based reinforcement learning to generate diverse reasoning-response pairs.
		</p>
              </td>
	 </tr>
	<!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/robobrain.jpg' width="180" height="110">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://superrobobrain.github.io/">
                  <papertitle>RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete</papertitle>
                </a>
                <br>
	        <strong>Yuheng Ji</strong>*,
		Huajie Tan*,
	        Jiayu Shi*,
		Xiaoshuai Hao*,
		Yuan Zhang,
	        Hengyuan Zhang,
	        Pengwei Wang,
	        Mengdi Zhao,
		Yao Mu,
		Pengju An,
		Xinda Xue,
		Qinghang Su,
		Huaihai Lyu,
		Xiaolong Zheng,
		Jiaming Liu,
		Zhongyuan Wang,
		Shanghang Zhang
                <br>
                <em>CVPR</em>, 2025
                <br>
		<a href="https://superrobobrain.github.io/">Project</a>
		/
		<a href="https://superrobobrain.github.io/">Paper</a>
                <p></p>   
                <p style="text-align:justify">
                We developed <strong>RoboBrain</strong>, an MLLM-based model that combines robotic and general multi-modal data, utilizes a multi-stage training strategy, 
			and incorporates long videos and high-resolution images to improve its robotic manipulation capabilities. Extensive experiments demonstrate that RoboBrain achieves state-of-the-art performance across various robotic tasks,
			highlighting its potential to advance robotic brain capabilities.
		</p>
              </td>
	 </tr>
	<!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/msc-bench.png' width="180" height="100">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://msc-bench.github.io/">
                  <papertitle>MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception</papertitle>
                </a>
                <br>
		Xiaoshuai Hao,
		Guanqun Liu,
		Yuting Zhao,
                <strong>Yuheng Ji</strong>,
                Mengchuan Wei,
                Haimei Zhao,
                Lingdong Kong,
		Rong Yin,
		Yu Liu
                <br>
                <em>ICME</em>, 2025
                <br>
		<a href="https://msc-bench.github.io/">Project</a>
		/
		<a href="https://arxiv.org/abs/2501.01037">Paper</a>
                <p></p>   
                <p style="text-align:justify">
                This work introduces Multi-Sensor Corruption Benchmark (MSC-Bench), 
		the first comprehensive benchmark aimed at evaluating the robustness of multi-sensor autonomous driving perception models against various sensor corruptions.
		</p>
              </td>
	 </tr>
	<!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/MinGRE.jpg' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2404.13425">
                  <papertitle>Alleviating Performance Disparity in Adversarial Spatiotemporal Graph Learning under Zero-inflated Distribution</papertitle>
                </a>
                <br>
		Songran Bai,
                <strong>Yuheng Ji</strong>,
                Yue Liu,
                Xingwei Zhang,
                Xiaolong Zheng,
		Daniel Dajun Zeng
                <br>
                <em>AAAI (<font color="red">Oral</font>)</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2404.13425">Paper</a>
                <p></p>
                <p style="text-align:justify">
                Spatiotemporal Graph Learning (SGL) under Zero-Inflated Distribution (ZID) is vital for urban risk management but is susceptible to adversarial attacks. 
			Traditional adversarial training (AT) increases performance disparities between classes. 
			We propose the MinGRE framework to reduce these disparities and enhance robustness, promoting more equitable and robust models.
		</p>
              </td>
	 </tr>
	 <!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/advlora.jpg' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2404.13425">
                  <papertitle>Enhancing Adversarial Robustness of Vision-Language Models through Low-Rank Adaptation</papertitle>
                </a>
                <br>
                <strong>Yuheng Ji</strong>*,
                Yue Liu*,
                Zhicheng Zhang,
                Zhao Zhang,
                Yuting Zhao,
		            Xiaoshuai Hao,
                Gang Zhou,
                Xingwei Zhang,
                Xiaolong Zheng
                <br>
                <em>ICMR</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2404.13425">Paper</a>
                <p></p>
                <p style="text-align:justify">
                We propose a parameter-efficient adversarial adaptation method named AdvLoRA by low-rank adaptation to improve the robustness of vision-language models.
                </p>
              </td>
	 </tr>
	 <!--------------------------------------------------------------------------------------------------------------------------->
   <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/FastRSR.jpg' width="180" height="90">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2404.13425">
        <papertitle>FastRSR: Efficient and Accurate Road Surface Reconstruction from Bird's Eye View</papertitle>
      </a>
      <br>
      Yuting Zhao*,
      <strong>Yuheng Ji</strong>*,
      Xiaoshuai Hao,
      Shuxiao Li
      <br>
      <em>arXiv</em>, 2025
      <br>
      <a href="https://arxiv.org/abs/2504.09535">Paper</a>
      <p></p>
      <p style="text-align:justify">
      Road Surface Reconstruction (RSR) is crucial for autonomous driving, enabling the understanding of road surface conditions.
      Traditional BEV-based methods for transforming perspective views to BEV face challenges such as information loss and representation sparsity.
      We present two innovative BEV-based RSR models: FastRSR-mono and FastRSR-stereo, offering superior efficiency and accuracy, achieving state-of-the-art results in elevation absolute error and processing speed.
</p>
    </td>
</tr>
	 <!--------------------------------------------------------------------------------------------------------------------------->
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/CCMH.jpg' width="180" height="90">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-981-99-9021-4_48">
                <papertitle>Learning Hash Subspace from Large-Scale Multi-modal Pre-Training: A CLIP-Based Cross-modal Hashing Framework</papertitle>
              </a>
              <br>
              <strong>Yuheng Ji</strong>*,
              Xingwei Zhang*,
              Gang Zhou,
              Xiaolong Zheng,
              Daniel Dajun Zeng
              <br>
              <em>China Conference on Command and Control (<font color="red">Outstanding Paper Award</font>)</em>, 2023
              <br>
	      <a href="https://link.springer.com/chapter/10.1007/978-981-99-9021-4_48">Paper</a>
              <p></p>
              <p style="text-align:justify">
              We propose a cross-modal hashing framework called CCMH (CLIP-based Cross-Modal Hashing), which facilitates the transferability of a well-trained real-value semantic subspace to a hash semantic subspace.
              </p>
            </td>
	</tr>
<!--------------------------------------------------------------------------------------------------------------------------->
<!--paper_list-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li>Remote Visiting Student @ <a href="https://www.nus.edu.sg/"> National University of Singapore (NUS)</a>, working with Ph.D. <a href="https://yueliu1999.github.io/">Yue Liu</a></li>
	    <li>Remote Visiting Student, supervised by Dr. <a href="https://scholar.google.com/citations?user=2xR6P5AAAAAJ&hl=zh-CN&oi=ao">Pengwei Wang</a> and Dr. <a href="https://scholar.google.com/citations?user=ui0lvY4AAAAJ&hl=en&oi=ao">Xiaoshuai Hao</a> </li>
            <li>Master Student @ <a href="http://www.ia.cas.cn/"> Chinese Academy of Science, Institute of Automation (CASIA)</a>, supervised by Prof. <a href="https://people.ucas.edu.cn/~xlzheng">Xiaolong Zheng</a></li>
            <li>Bachelor of Engineering @ <a href="http://english.neu.edu.cn/">Northeastern University</a>, supervised by Prof. Miao Fang</li>
            </ul>
        </table>
        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
      <li>Reviewer for ICMR'25</li>
	    <li>Reviewer for ICME'25</li>
	    <li>Reviewer for CVPR'25</li>
	    <li>Reviewer for ICLR'25</li>
            </ul>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Award</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li>[2024] Merit Student, UCAS, School Award.</li>
            <li>[2023] Outstanding Graduates, Provincial Award.</li>
            <li>[2022] Recommendation for admission to CASIA.</li>
            <li>[2022] Merit Student, Provincial Award.</li>
            <li>[2022] China National Scholarship for Undergraduate Student, National Award.</li>
            <li>[2021] China National Scholarship for Undergraduate Student, National Award.</li>
            <li>[2020] China National Scholarship for Undergraduate Student, National Award.</li>
            <li>[2019-2023] Scholarships, School Award.</li>
            </ul>
        </table>


	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Others</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li>[2024] ÂÜÄÊò±Ë°°, Âº†Êõå, ÈÉëÊôìÈæô, "Â§ßÊ®°ÂûãÂæÆË∞É‰∏≠ÁöÑ‰ΩéÁß©ÊÄß," ‰∏≠ÂõΩÊåáÊå•‰∏éÊéßÂà∂Â≠¶‰ºöÈÄöËÆØ 55 (1), 44-49.</li>
            <li>[2023] ÂÜÄÊò±Ë°°, Âº†ÂÖ¥‰ºü, ÈÉëÊôìÈæô, "Âü∫‰∫éÂ§öÊ®°ÊÄÅÈ¢ÑËÆ≠ÁªÉÁöÑË∑®Ê®°ÊÄÅÊ£ÄÁ¥¢ÁÆóÊ≥ïÁ†îÁ©∂," ‰∏≠ÂõΩÊåáÊå•‰∏éÊéßÂà∂Â≠¶‰ºöÈÄöËÆØ 46 (4), 10-16.</li>
            <li>[2023] ‰∏ÄÁßçÂü∫‰∫éÂ§öÊ®°ÊÄÅÈ¢ÑËÆ≠ÁªÉÁöÑË∑®Ê®°ÊÄÅÂìàÂ∏åÊ£ÄÁ¥¢Á≥ªÁªüÔºåÂèëÊòé‰∏ìÂà©ÔºåÁ¨¨‰∏ÄÂèëÊòé‰∫∫</li>
            <li>[2023] ‰∏ÄÁßçÂü∫‰∫éÂõæÁ•ûÁªèÁΩëÁªúÁöÑ‰ø°Áî®Âç°Ê¨∫ËØàÊ£ÄÊµãÁ≥ªÁªüÔºåÂèëÊòé‰∏ìÂà©ÔºåÁ¨¨‰∏ÄÂèëÊòé‰∫∫</li>
            <li>[2023] ‰∏ÄÁßçÈíàÂØπÊ£ÄÁ¥¢Ê®°ÂûãÁöÑÂú®Á∫øÈöêÁßÅ‰øùÊä§Á≥ªÁªüÔºåÂèëÊòé‰∏ìÂà©ÔºåÁ¨¨‰∫åÂèëÊòé‰∫∫</li>
            <li>[2022] ‰∏ÄÁßçÂü∫‰∫éÊñ∞Èóª‰∏ªÈ¢òÂè•ÁöÑÊñáÊú¨ÊÉÖÊÑüÂàÜÁ±ªÁ≥ªÁªüÔºåÂèëÊòé‰∏ìÂà©ÔºåÁ¨¨‰∫åÂèëÊòé‰∫∫</li>
            </ul>
        </table>

 	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Participation in Research Projects</heading>
	      <p style="margin-bottom: 0.5px;">
                Âú®ÊîªËØªÁ°ïÂçöÊúüÈó¥ÂèÇ‰∏é‰∫Ü‰ª•‰∏ãÈ°πÁõÆÁ†îÁ©∂Ôºå‰∏ªË¶ÅË¥üË¥£È°πÁõÆ‰∏≠Ë∑®Ê®°ÊÄÅ‰ø°ÊÅØËØ≠‰πâËûçÂêà‰∏éÁêÜËß£Á≠â‰∏ìÈ¢òÁ†îÁ©∂Â∑•‰ΩúÔºö
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li> Âü∫‰∫éÂ§öÊ®°ÊÄÅÊï∞ÊçÆËûçÂêàÁöÑÊô∫ËÉΩÁ§æ‰ºöÈ£éÈô©È¢ÑË≠¶Á†îÁ©∂, ÂõΩÂÆ∂Ëá™ÁÑ∂ÁßëÂ≠¶Âü∫ÈáëÈáçÁÇπÈ°πÁõÆ.</li>
	    <li> Êñ∞ÊäÄÊúØÈ©±Âä®ÁöÑÂ§çÊùÇÁ§æ‰ºöÁ≥ªÁªüÁÆ°ÁêÜ, ÂõΩÂÆ∂Êù∞Âá∫ÈùíÂπ¥ÁßëÂ≠¶Âü∫ÈáëÈ°πÁõÆ.</li>
	    <li> ‰ø°ÊÅØÊäÄÊúØÊîØÊíëÂõΩÂÆ∂Ê≤ªÁêÜÁé∞‰ª£ÂåñÁöÑÊàòÁï•Á†îÁ©∂, ‰∏≠ÂõΩÁßëÂ≠¶Èô¢Â≠¶ÈÉ®ÈáçÂ§ßÂí®ËØ¢È°πÁõÆ.</li>
	    <li> Ë∑®Ê®°ÊÄÅÂ§öËØ≠Ë®ÄÂ§ßÊï∞ÊçÆÈ©±Âä®ÁöÑÁ§æ‰ºöÈ£éÈô©ÊÑüÁü•‰∏éÁêÜËß£, 2030‚Äî‚ÄúÊñ∞‰∏Ä‰ª£‰∫∫Â∑•Êô∫ËÉΩ‚ÄùÈáçÂ§ßÈ°πÁõÆ.</li>
            </ul>
        </table>
        

        <script>
          let show = false;
          document.querySelector('#projects-show').onclick = function() {
            if (!show) {
              document.querySelector('#projects-box').style.height = 'auto';
              document.querySelector('#projects-box').style.paddingBottom = '20px';
              document.querySelector('#projects-show-text').innerHTML = 'Less';
              show = true;
            } else {
              show = false;
              document.querySelector('#projects-box').style.height = '160px';
              document.querySelector('#projects-box').style.paddingBottom = '0px';
              document.querySelector('#projects-show-text').innerHTML = 'More';
            }
          }
        </script>
</body>
</html>
